{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEK5030_deep_learning_EX1_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1SfP9VMrXICPVGSdWSDxpJ8kpr1VwFsiP",
      "authorship_tag": "ABX9TyM1da+hHt6G180a3h53Bogw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sigmunjr/TEK5030_deep_learning/blob/master/TEK5030_deep_learning_EX1_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IiXB_9liRw2",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 1: Simple Convolutional Neural network (CNN)\n",
        "\n",
        "In this execrise you are building a neural network to separate between cats and dogs. If you want, you can also try separating different breeds.\n",
        "\n",
        "## Dataset\n",
        "We will use [Tensorflow Datasets](https://www.tensorflow.org/datasets) in this exercise. This makes it fast and easy to download and get started. You can also input numpy arrays directly or build your own data pipelines using [tf.data](https://www.tensorflow.org/guide/data).\n",
        "\n",
        "We use the [oxford_iiit_pet](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet) dataset, that provide images of dogs and cats and labels that say whether it is a cat or a dog, the breed of the animal and a segmentation mask.\n",
        "\n",
        "### Loading and preprocessing the data\n",
        "A [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) has serveral transform functions. We use [map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) to extract only the data we want and preprocess the images. We resize the images for faster processing and subtrackt 1 from input mask to get the values [0, 1].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3AyUHgcZwKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "#tfds.disable_progress_bar()\n",
        "\n",
        "def load_image(datapoint):\n",
        "  input_image = tf.cast(tf.image.resize(datapoint['image'], (128, 128)), tf.float32) / 255.\n",
        "  input_mask = tf.image.resize(\n",
        "      datapoint['segmentation_mask'], (128, 128), 'nearest'\n",
        "      ) - 1\n",
        "  print('Data in datapoint:', list(datapoint.keys()))\n",
        "\n",
        "  return input_image, input_mask, datapoint['species']\n",
        "\n",
        "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True, download_and_prepare_kwargs={'download_config': tfds.download.DownloadConfig(register_checksums=True)})\n",
        "train_data = dataset['train'].map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "test_data = dataset['test'].map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NY7_HKsmHAM",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing the data\n",
        "You should always inspect the data your using as training data, to get a feeling for the problem. \n",
        "\n",
        "Her we print the plot the image, the mask and its label.\n",
        "\n",
        "**TODO:** Print the type of animal and not just the number for each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH4lmzJ4fNUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display(display_list, titles=None):\n",
        "  \"\"\" Plotting images in list \"\"\"\n",
        "  from matplotlib import cm\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    if titles != None:\n",
        "      plt.title(titles[i])\n",
        "    plt.imshow(\n",
        "        tf.keras.preprocessing.image.array_to_img(display_list[i]),\n",
        "        vmin=0,\n",
        "        vmax=1\n",
        "        )\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "for image, mask, label in train_data.take(4):\n",
        "  # TODO: Print tekst string with animal type.\n",
        "  print('label:', label.numpy())\n",
        "  display([image, mask])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1In7K8aElaJ",
        "colab_type": "text"
      },
      "source": [
        "### Nearest neighbors in pixel space\n",
        "\n",
        "For fun we can check out which images are close in pixel space. This means we will find images with similar colored pixels in the same location. Often generally dark images will match with dark images and visa-versa. This makes it clear that we cannot separate cat and dogs with a shallow classification algorithm in pixel space directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R44wH4-uYB5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Pick the first 200 samples and take only the images of each sample\n",
        "only_images_dataset = train_data.take(600).map(lambda img, i1, i2: img)\n",
        "\n",
        "# Extract the images to a numpy array of the 400 images of shape 200x128x128x3\n",
        "X = np.array(list(only_images_dataset.as_numpy_iterator()))\n",
        "\n",
        "nbrs = NearestNeighbors(n_neighbors=3)\n",
        "# Add images reshaped to vectors to nearest-neighbors tree\n",
        "nbrs.fit(X.reshape([X.shape[0], -1]))\n",
        "\n",
        "# Loop through 10 test images and display 3 nearest neighbors\n",
        "for image, mask, label in test_data.shuffle(100).take(20):\n",
        "  distances, indices = nbrs.kneighbors(image.numpy().reshape((1, -1)))\n",
        "  show_img = display([\n",
        "                      image.numpy(),\n",
        "                      X[indices[0, 0]],\n",
        "                      X[indices[0, 1]],\n",
        "                      X[indices[0, 2]]\n",
        "                      ], ['Image'] + list(distances[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t25zQpioTCOP",
        "colab_type": "text"
      },
      "source": [
        "## Fully-convolutional network\n",
        "It's a good idea to implement the network as a fully-convolutional neural network. This means that the network will work for all input sizes.\n",
        "\n",
        "A fully-convolutional network use no fully-connected layers (matrix multiplications). They often consist of mostly convolutional layers, but also include other operations that does not require fixed input size like: add, concatenations, max (relu), batch normalization etc.\n",
        "\n",
        "## Keras model\n",
        "I recommend using the Object-Oriented style of Keras, as it is the most flexible and are almost identical in both Keras/Tensorflow and PyTorch. In this approach you make subclass of [tensorflow.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) and implement the **\\_\\_init\\_\\_** and **call** method. In **\\_\\_init\\_\\_** you need to initialize everything that has state in your network, while **call** method actually run the network from output to input.\n",
        "\n",
        "**TODO:**\n",
        "Fill out the *\\_\\_init\\_\\_* and *call* method with convolutional layers and use the [*ReLU*](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) as activation function.\n",
        "\n",
        "The output of the *call* function should have shape \\[*batch_size*, *num_classes*\\].\n",
        "\n",
        "**If the \"output_features\" parameter is *True* you should output the result before the last convolutioal layer.**\n",
        "\n",
        "*HINT: You can use [tf.math.reduce_mean](https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean) to sum over the spatia* dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKAAblBdfp5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import Model\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout\n",
        "\n",
        "\n",
        "class SimpleNet(Model):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # TODO: Initialize the layers of your network\n",
        "        # You can find different layers in tensorflow.keras.layers (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers)\n",
        "        self.conv1 = Conv2D(32, 5, strides=2, activation=tf.nn.relu, padding='valid')\n",
        "        self.conv2 = Conv2D(64, 5, strides=2, activation=tf.nn.relu, padding='valid')\n",
        "        self.conv3 = Conv2D(128, 5, strides=2, activation=tf.nn.relu, padding='valid')\n",
        "        self.conv4 = Conv2D(128, 5, strides=2, activation=tf.nn.relu, padding='valid')\n",
        "        self.conv5 = Conv2D(128, 5, strides=1, activation=tf.nn.relu, padding='valid')\n",
        "        self.dropout = Dropout(0.5)\n",
        "\n",
        "        self.conv6 = Conv2D(num_classes, 1, strides=1, activation=None, padding='valid')\n",
        "\n",
        "    def call(self, x, visualise=False, output_features=False, training=False):\n",
        "        # TODO: Run the image through your network\n",
        "        # Your input should be a [Batch_size x 3 x 32 x 32] sized tensor\n",
        "        # Your output should be a [Batch_size x num_classes] sized matrix\n",
        "        #x -= tf.math.reduce_mean(x)\n",
        "        #x /= tf.math.reduce_variance(x)\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        if output_features: return x\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.conv6(x)\n",
        "        if visualise:\n",
        "            pass\n",
        "        #x = tf.reshape(x, [x.shape[0], -1])\n",
        "        #x = self.fc1(x)\n",
        "        #x = self.fc2(x)\n",
        "        #x = self.fc3(x)\n",
        "        # Return the result of your network\n",
        "        return tf.math.reduce_mean(x, axis=(1, 2))\n",
        "\n",
        "# Initializing the model\n",
        "NUM_CLASSES = 5\n",
        "BATCH_SIZE = 8\n",
        "model = SimpleNet(NUM_CLASSES)\n",
        "# Running the model with 8 random 128x128x3 images\n",
        "model_output = model(np.random.random((BATCH_SIZE, 128, 128, 3)).astype(np.float32))\n",
        "print('Model output:', model_output)\n",
        "print('Model output shape:', model_output.shape)\n",
        "\n",
        "assert model_output.shape == (BATCH_SIZE, NUM_CLASSES), \"Incorrect output size from call\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPXhw3XYPNAS",
        "colab_type": "text"
      },
      "source": [
        "### Training a Keras model\n",
        "There is basically two different methods of training a Tensorflow-Keras model. \n",
        "\n",
        "One is **simple** method is just calling the models [compile](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) and [fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) functions. This is simple and makes sure everything is handled properly, but it leaves the control to Keras, so making changes in the training process can only be done by providing a list of [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks) to **fit**.\n",
        "\n",
        "For the second method you take control over the training process yourself. You can check out [this guide](https://www.tensorflow.org/tutorials/quickstart/advanced) for a good example. Here there are three fundemental steps to you training: Creating an optimizer, run model to extract gradients and apply gradients.\n",
        "\n",
        "Creating an optimizer\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "Run model to extract gradients\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = model(input)\n",
        "    \n",
        "Running the model inside the GradientTape scope stores the gradients in \"tape\".\n",
        "\n",
        "Apply gradients\n",
        "\n",
        "    # Get gradients of loss with respect to the trainable_variables\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    \n",
        "    # Run optimization step following the optimizers scheme\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "This second method is good for debuging or if you need some specialized form of training. For larger projects i recommend implementing both ways of training, one for debuging and one for running.\n",
        "\n",
        "#### **Tips for training**\n",
        "Start by training your network with just a small dataset, e.g. one batch. \n",
        "\n",
        "To check you have properly initialized your weights and scaled your inputs you can check your initial loss. Your initial loss should be roughly equal to *-ln(1/N)*, where N is the number of classes.\n",
        "\n",
        "When training on a small version of your dataset your accuracy should steadily rise to 1. If it does not, you could try to adjust your learning rate. Your learning rate should generally be as high as possible, while still not increase your loss at the begining of your training.\n",
        "\n",
        "Adam is often a good optimizer for quick tests, as it requires comparably less *tuning* of the learning rate.\n",
        "\n",
        "**TODO**:\n",
        "Train the network to a decent test accuracy. The dataset are small so we cannot expect very good accuracy, but you should at least expect 0.72 test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vM7LSpWk8Jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SimpleNet(2)\n",
        "\n",
        "#for image, mask, label in train_data.take(20):\n",
        "#  print(model(image[tf.newaxis]).shape, label)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        "    run_eagerly=False,\n",
        "    )\n",
        "train_ds = train_data.map(lambda img, mask, label: (img, label)).shuffle(256, reshuffle_each_iteration=True)\n",
        "test_ds = test_data.map(lambda img, mask, label: (img, label))\n",
        "\n",
        "print('EVALUATE', model.evaluate(train_ds.take(32).batch(32)))\n",
        "model.fit(train_ds.batch(64), epochs=13, validation_data=test_ds.take(128).batch(64))\n",
        "model.evaluate(test_ds.batch(64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1mRoOulECt3",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing nearest neighbor in feature space\n",
        "\n",
        "The neural network transforms the input so that cats and dogs can be separated. Generally ths will make cat be close to cats and dogs be close to dogs. We can often also see that close object share some characteristics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak2cQArgvPB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Pick the first 200 samples and take only the images of each sample\n",
        "only_images_dataset = train_data.take(400).map(lambda img, i1, i2: img)\n",
        "images_np = only_images_dataset.batch(400).__iter__().__next__().numpy()\n",
        "# Extract the images to a numpy array of the 200 images of shape 200x128x128x3\n",
        "X = model(images_np, output_features=True).numpy()\n",
        "\n",
        "\n",
        "nbrs = NearestNeighbors(n_neighbors=3)\n",
        "# Add images reshaped to vectors to nearest-neighbors tree\n",
        "print('XSHAPE', X.shape)\n",
        "nbrs.fit(X.reshape([X.shape[0], -1]))\n",
        "\n",
        "# Loop through 10 test images and display 3 nearest neighbors\n",
        "for image, mask, label in test_data.shuffle(1000).take(20):\n",
        "  distances, indices = nbrs.kneighbors(model(image[tf.newaxis], output_features=True).numpy().reshape((1, -1)))\n",
        "  show_img = display([\n",
        "                      image.numpy(),\n",
        "                      images_np[indices[0, 0]],\n",
        "                      images_np[indices[0, 1]],\n",
        "                      images_np[indices[0, 2]]\n",
        "                      ], ['Image'] + list(distances[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SttjNGvMtdgL",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing [GRAD-CAM](http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf)\n",
        "\n",
        "There are many techniques for visualizing what is going on inside a neural network.\n",
        "\n",
        "The technique use the gradients of the output label with respect to the image in combination with the values of the activation map. With this information we can plot what regions of the input that are pointing toward a given label.\n",
        "\n",
        "We would e.g. expect that a cat would light up if we used the cat label for as input to Grad-CAM.\n",
        "\n",
        "With this technique we can check if the network actually use relvant information in the classification process and are not overfitted.\n",
        "\n",
        "Since the output is normalized, it will always highlight some regions. If the surroundings is highlighted it may simply mean that it found nothing \"doglike\" in the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pyOt6U0tcNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-keras-vis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ4ck6CUtjkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vis.utils import utils\n",
        "from tf_keras_vis.gradcam import Gradcam\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, models\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "\n",
        "def convert_to_functional_model(model):\n",
        "  input_layer = layers.Input(batch_shape=[1, 128, 128, 3])\n",
        "  prev_layer = input_layer\n",
        "\n",
        "  def iterate_model_layers(model, prev_layer):\n",
        "    for layer in model.layers:\n",
        "      prev_layer = layer(prev_layer)\n",
        "    return prev_layer\n",
        "\n",
        "  prev_layer = iterate_model_layers(model, prev_layer)\n",
        "  return models.Model([input_layer], [prev_layer])\n",
        "\n",
        "# We need to convert the model to functional graph to use tf-keras-vis\n",
        "funcmodel = convert_to_functional_model(model)\n",
        "\n",
        "#Create gradcam for model\n",
        "gradcam = Gradcam(funcmodel)\n",
        "\n",
        "def plot_gradcam_for_layer(ax, image, label):\n",
        "  label_map = ['CAT', 'DOG']\n",
        "  def loss(output):\n",
        "    area_mean = tf.math.reduce_mean(output, axis=[0, 1, 2])\n",
        "    estimated_label = tf.argmax(area_mean)\n",
        "    print('Estimated', label_map[estimated_label],\n",
        "          'with probability:', tf.nn.softmax(area_mean)[estimated_label].numpy())\n",
        "    return area_mean[label]\n",
        "\n",
        "  image = tf.image.resize(image[tf.newaxis], [256, 256])\n",
        "  cam = gradcam(loss, image, penultimate_layer=7)\n",
        "  #Normalize output\n",
        "  cam -= cam.min()\n",
        "  cam /= cam.max() if cam.max() > 0. else 1.0 \n",
        "\n",
        "  ax[label].set_title(label_map[label])\n",
        "  ax[label].imshow(image[0])\n",
        "  ax[label].imshow(cam[0], cmap='jet', alpha=0.8*cam[0])\n",
        "\n",
        "for image, mask, label in test_data.take(20):\n",
        "  f, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
        "  plot_gradcam_for_layer(ax, image, 0)\n",
        "  plot_gradcam_for_layer(ax, image, 1)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmiq-vx6PdZ9",
        "colab_type": "text"
      },
      "source": [
        "### Finetuning a network\n",
        "You can quite easily download and run a pretrained network. Using a network that is trained on a similar, but larger dataset will often improve your result. Using a network pretrained on the ImageNet dataset will definetly work, as the dataset already includes different types of dogs and cats.\n",
        "\n",
        "You can e.g. load a pretrained network like this:\n",
        "\n",
        "    net = MobileNetV2(weights='imagenet', include_top=False)\n",
        "\n",
        "Here we exclude the top classification layer, as we want a different number of classes. So we have to make this layer for our self.\n",
        "\n",
        "Now the network can be used similar to a Conv2D layer. If you have a small dataset or few computational resourses, you can freeze the network parameters in the pretrained network like this:\n",
        "\n",
        "    net.trainable = False\n",
        "\n",
        "**TODO:**\n",
        "Use a pretrained network as a start of your model, but make a few convolutional layers for yourself. Train only the last layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tirX2oB_IZ_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import Model\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "class FinetuneNet(Model):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(FinetuneNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # TODO: Initialize the layers of your network\n",
        "        # You can find different layers in tensorflow.keras.layers (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers)\n",
        "        self.base_network = MobileNetV2(weights='imagenet', include_top=False)\n",
        "        self.base_network.trainable = False\n",
        "        self.conv1 = Conv2D(256, 1, strides=1, activation=None, padding='valid')\n",
        "        self.dropout = Dropout(0.5)\n",
        "\n",
        "        self.last_layer = Conv2D(num_classes, 1, strides=1, activation=None, padding='valid')\n",
        "\n",
        "    def call(self, x, visualise=False, output_features=False, training=False):\n",
        "        # TODO: Run the image through your network\n",
        "        # Your input should be a [Batch_size x 3 x 32 x 32] sized tensor\n",
        "        # Your output should be a [Batch_size x num_classes] sized matrix\n",
        "        x = self.base_network(x)\n",
        "        x = self.conv1(x)\n",
        "        if output_features: return x\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.last_layer(x)\n",
        "        if visualise:\n",
        "            pass\n",
        "        # Return the result of your network\n",
        "        return tf.math.reduce_mean(x, axis=(1, 2))\n",
        "\n",
        "# Initializing the model\n",
        "NUM_CLASSES = 5\n",
        "BATCH_SIZE = 8\n",
        "model2 = FinetuneNet(NUM_CLASSES)\n",
        "# Running the model with 8 random 128x128x3 images\n",
        "model_output = model2(np.random.random((BATCH_SIZE, 128, 128, 3)).astype(np.float32))\n",
        "print('Model output:', model_output)\n",
        "print('Model output shape:', model_output.shape)\n",
        "\n",
        "assert model_output.shape == (BATCH_SIZE, NUM_CLASSES), \"Incorrect output size from call\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_ekQr68aGNq",
        "colab_type": "text"
      },
      "source": [
        "**TODO:**\n",
        "\n",
        "Train your new network.\n",
        "\n",
        "This should give a lot better performance, at least 0.9 in test accuracy!\n",
        "\n",
        "You can re-run the visualization code, with this model. This should give a better clustering of dog and cats, and even cluster different breeds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDe4uGErMXvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = FinetuneNet(2)\n",
        "\n",
        "#for image, mask, label in train_data.take(20):\n",
        "#  print(model(image[tf.newaxis]).shape, label)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        "    run_eagerly=False,\n",
        "    )\n",
        "train_ds = train_data.map(lambda img, mask, label: (img, label)).shuffle(256, reshuffle_each_iteration=True)\n",
        "test_ds = test_data.map(lambda img, mask, label: (img, label))\n",
        "\n",
        "print('EVALUATE', model.evaluate(train_ds.take(32).batch(32)))\n",
        "model.fit(train_ds.batch(64), epochs=10, validation_data=test_ds.take(128).batch(64))\n",
        "model.evaluate(test_ds.batch(64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzcIUW4AiVpR",
        "colab_type": "text"
      },
      "source": [
        "## Creating adverserial image\n",
        "Find an image that fools the classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cven21cKiRAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_img, sample_label = test_ds.make_one_shot_iterator().next()\n",
        "loss_func = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "zero_img = np.ones_like(sample_img.numpy())\n",
        "\n",
        "label_map = ['CAT', 'DOG']\n",
        "display([sample_img, zero_img], [label_map[sample_label], ''])\n",
        "print(label_map[model(sample_img[tf.newaxis]).numpy().argmax()])\n",
        "\n",
        "tmp_img = sample_img\n",
        "for i in range(10):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(tmp_img)\n",
        "    pred = model(tmp_img[tf.newaxis])\n",
        "    loss = loss_func([0.], pred)\n",
        "  img_grad = tape.gradient(loss, tmp_img)\n",
        "  tmp_img -= 0.01*img_grad\n",
        "  display([tmp_img, zero_img], [label_map[pred.numpy().argmax()] + ': ' + str(loss), ''])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAT8rTwEBtUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}