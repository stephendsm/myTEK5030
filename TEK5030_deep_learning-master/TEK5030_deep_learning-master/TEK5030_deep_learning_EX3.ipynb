{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEK5030_deep_learning_EX3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMqEnr6QesxyQhD0CPiiVNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sigmunjr/TEK5030_deep_learning/blob/master/TEK5030_deep_learning_EX3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDj-z0aWYva8",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 3: Segmentation\n",
        "\n",
        "We use the same dataset as in *Exercise 1*, but now instead of classifying dogs vs. cats, we try to output a segmentation mask. In other words simply classify each pixel, for this dataset simply into *animal* and not *animal*.\n",
        "\n",
        "First we load the dataset, just as last time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC8hof3oTHKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "def load_image(datapoint):\n",
        "  input_image = tf.cast(tf.image.resize(datapoint['image'], (128, 128)), tf.float32) / 255.\n",
        "  print(datapoint['segmentation_mask'].dtype)\n",
        "  input_mask = tf.math.minimum(tf.image.resize(\n",
        "      tf.cast(datapoint['segmentation_mask'], tf.int64), (128, 128), 'nearest'\n",
        "      ) - 1, 1)\n",
        "  print('Data in datapoint:', list(datapoint.keys()))\n",
        "\n",
        "  return input_image, input_mask, datapoint['species']\n",
        "\n",
        "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n",
        "train_data = dataset['train'].map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "test_data = dataset['test'].map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbgD24bZaAA0",
        "colab_type": "text"
      },
      "source": [
        "## Creating a segmentation network\n",
        "\n",
        "A segmentation network is very similar to a classification network, except that your output need to be in the same spatial dimentions as your input.\n",
        "\n",
        "### Simple segmentation network\n",
        "The simplest way of doing this is to run a fully-convolutional network (only convolutionas and per-pixel operations). This approach was first proposed by the paper [Fully Convolutional Networks for Semantic Segmentation](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf).\n",
        "\n",
        "You need to make sure you have some spatiel information left (If the shape of your output is Batch-size*N*M*C, then N and M should be larger than 1 (probably N, M > 5).\n",
        "\n",
        "I use [tf.image.resize](https://www.tensorflow.org/api_docs/python/tf/image/resize) to resize the output of the network. This implements gradients, so we can run backpropagation through the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s882i-jRSxY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout\n",
        "\n",
        "class SmallSegmentationNet(Model):\n",
        "    def __init__(self):\n",
        "        super(SmallSegmentationNet, self).__init__()\n",
        "        # TODO: Intialize the components of the network\n",
        "\n",
        "    def call(self, x, **kwargs):\n",
        "        in_size = x.shape\n",
        "        # TODO: Run your network\n",
        "\n",
        "        return tf.image.resize(x, in_size[1:3])\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(pred, label):\n",
        "      #TODO: Write your own loss function. Use it with SmallSegmentationNet.loss\n",
        "      return 0.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2EvZV4Pgxr-",
        "colab_type": "text"
      },
      "source": [
        "## Train your network\n",
        "\n",
        "Luckily enough Keras have at this point support for using [tf.keras.losses.SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy), for segmentation. So you can use this out of the box as long as your outputs final dimenstion size is equal to the number of classes.\n",
        "\n",
        "**TODO:**\n",
        "- Train your network with SparseCategoricalCrossentropy\n",
        "- Try to make your own loss function.\n",
        "\n",
        "Creating your own loss function with tf.GradientTape just mean that you have to output some value, that you want to optimize. Then calculate your gradients to that value, with tape.gradient(*YOUR_VALUE*, model.trainable_variables).\n",
        "\n",
        "If your are using the *model.fit* method, you need to provide a function that takes the network output and target values. Example:\n",
        "\n",
        "    def my_loss_function(network_output, target_values):\n",
        "      return tf.math.reduce_sum(network_output * target_values)\n",
        "    \n",
        "    model.compile(\n",
        "      ...,\n",
        "      loss=my_loss_function,\n",
        "      ...\n",
        "      )\n",
        "\n",
        "The function should simpliy return the value you want to minimize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoZ-2O4OTX7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SmallSegmentationNet()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        "    run_eagerly=False,\n",
        "    )\n",
        "train_ds = train_data.map(lambda img, mask, label: (img, mask)).shuffle(256, reshuffle_each_iteration=True)\n",
        "test_ds = test_data.map(lambda img, mask, label: (img, mask))\n",
        "\n",
        "print('EVALUATE', model.evaluate(train_ds.take(32).batch(32)))\n",
        "model.fit(train_ds.batch(64), epochs=13, validation_data=test_ds.take(128).batch(64))\n",
        "model.evaluate(test_ds.batch(64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14RjV5yBkZLH",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing your result\n",
        "\n",
        "**TODO**: Inside the loop iterating through the test data, run your model and plot the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YiR-6J3Vf1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display(display_list, titles=None, cmaps={}):\n",
        "  \"\"\" Plotting images in list \"\"\"\n",
        "  from matplotlib import cm\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    if titles != None:\n",
        "      plt.title(titles[i])\n",
        "    plt.imshow(\n",
        "        display_list[i],\n",
        "        vmin=0,\n",
        "        vmax=1,\n",
        "        cmap=cmaps[i] if i in cmaps else None\n",
        "        )\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "for image, mask, label in test_data.take(4):\n",
        "  # TODO: Get the output from your network\n",
        "  out = model(image[np.newaxis]).numpy()[0, :, :, 0]\n",
        "\n",
        "  #TODO: plot the output\n",
        "  display([image.numpy(), mask.numpy().squeeze(), out.squeeze()], cmaps={2: 'jet'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmk6D6Dnlf_T",
        "colab_type": "text"
      },
      "source": [
        "## Extra challenge\n",
        "\n",
        "Using the streigh forward fully-convolutional approach is simple, but you do lose much of the spatial information, makeing the network less precise. A better approach is to us a [U-Net](https://arxiv.org/pdf/1505.04597.pdf) type architecture. Here you make skip-connection, either by simple addition or by concatinating the outputs. This can be done by only changing the *call* method in the network. Then the output can be resized multiple times with tf.image.resize and the skip-connection can be done with [tf.concat](https://www.tensorflow.org/api_docs/python/tf/concat) or simply +. It is not necessary to use max-pooling or up-convolution/Conv2DTransposed.\n",
        "\n",
        "![u-net](https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41598-019-53797-9/MediaObjects/41598_2019_53797_Fig1_HTML.png)"
      ]
    }
  ]
}