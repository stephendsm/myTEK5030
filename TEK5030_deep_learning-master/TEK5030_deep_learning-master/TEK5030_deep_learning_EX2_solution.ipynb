{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEK5030_deep_learning_EX2_solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNuNP81lEbTndZU6I7cQI7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sigmunjr/TEK5030_deep_learning/blob/master/TEK5030_deep_learning_EX2_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6059iXi6rmDd",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 2: Live training\n",
        "\n",
        "This exercise is primarily so you can play with deep learning more directly. You need to use a webcamera to gather images in a dataset. Then you can run training and visualize your result iteratively.\n",
        "\n",
        "This first code block is just made to interact the notebook/website.  \n",
        "\n",
        "*   You can add new objects by typing the label name in the \"new object\" field and pressing enter\n",
        "*   After you have added some object they can be selected in the \"selected\" field\n",
        "*   You can add an image from your webcam with \"Add image to...\" button.\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "Add some labels and add a few images to each label.\n",
        "\n",
        "You may try to classify different objects you have laying around, or you can try to classify different hand gestures etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg0nrTF0_2H6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from webcam_in_notebook import WebCamera, LiveDataset, DatasetGUI\n",
        "except Error:\n",
        "  !pip install git+https://github.com/sigmunjr/webcam-in-notebook.git\n",
        "  from webcam_in_notebook import WebCamera, LiveDataset, DatasetGUI\n",
        "import tensorflow as tf\n",
        "\n",
        "dataset = LiveDataset()\n",
        "gui = DatasetGUI(dataset, WebCamera())\n",
        "gui.display()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkbOfBd3_Mq1",
        "colab_type": "text"
      },
      "source": [
        "## Create a network\n",
        "Create a network for classifying the images. It can be wise to use a pretrained net to get the best possible results.\n",
        "\n",
        "You should also consider setting most of the layers trainable to *false*, to make it work with as few images as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3K9UQONGjVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "\n",
        "model = MobileNetV2(weights='imagenet', include_top=True)\n",
        "for l in model.layers[:-1]:\n",
        "  l.trainable = False\n",
        "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "optimizer = tf.keras.optimizers.Adam(0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEi480dEEkgw",
        "colab_type": "text"
      },
      "source": [
        "Now it is time to train your network. Dataset have a method *get_batch* to fetch a batch of images and corresponding labels (You can check out the class at the bottom of this notebook).\n",
        "\n",
        "    images, labels = dataset.get_batch(16)\n",
        "\n",
        "With this you get a batch of 16 images and labels in the format of numpy arrays. If you use tf.GradientTape to train your networks, you can train as normal. If you use *fit* to train you model, you can either use [tf.data.Dataset.from_generator](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator) to train your model or you can simply:\n",
        "\n",
        "- get a large batch from the dataset\n",
        "- provide *x*, *y* and *batch_size* to model.fit\n",
        "\n",
        "To get best possible performance you may want to add some extra data augmentation, e.g. with the help of [tf.keras.preprocessing.image.ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator).\n",
        "\n",
        "**TODO:**\n",
        "Train you network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxK5e7tQApRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: train your network\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy'],\n",
        "    run_eagerly=False,\n",
        "    )\n",
        "images, labels = dataset.get_batch(16*10)\n",
        "model.fit(images, labels, batch_size=16, epochs=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6e-QV2pKgiH",
        "colab_type": "text"
      },
      "source": [
        "Finally you can test your network live. I have made a class for writing text on the web camera image *LiveJavascriptTextField*.\n",
        "\n",
        "**TODO:**\n",
        "Run your network on images from the webcamera to set *predicted_label* and *label_probablility*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEMApeJUQvUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_inference(camera):\n",
        "  from webcam_in_notebook import LiveJavascriptTextField\n",
        "  js_textfield = LiveJavascriptTextField(\n",
        "      'position: absolute;color: lightgreen; font-size: xx-large;'\n",
        "      )\n",
        "\n",
        "  for i, img in enumerate(camera):\n",
        "    # Todo: Run your network on images from the webcamera to set predicted_label and label_probablility\n",
        "    out = net(LiveDataset.convert_to_tf_image(img)[tf.newaxis])[0]\n",
        "    predicted_label = np.argmax(out.numpy())\n",
        "    output_probability = out[predicted_label]\n",
        "\n",
        "    dataset.name_map[predicted_label],\n",
        "    js_textfield.updateText(\"{}: {:.2f}\".format(\n",
        "        dataset.name_map[predicted_label],\n",
        "        output_probability\n",
        "    ))\n",
        "\n",
        "webcam = webcam_in_notebook.WebCamera()\n",
        "run_inference(webcam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvoQDI2UNVrh",
        "colab_type": "text"
      },
      "source": [
        "## Run visualization\n",
        "\n",
        "Finally you can try to run the visualization techniques from *Excercies 1* on images from you webcamera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4narTapNKxC",
        "colab_type": "text"
      },
      "source": [
        "## Code for GUI and dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9If3bLfJoFxj",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "from webcam_in_notebook import webcam_in_notebook\n",
        "import random\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "import threading\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display\n",
        "from IPython.display import HTML\n",
        "from IPython.display import clear_output\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "\n",
        "class LiveDataset:\n",
        "    def __init__(self):\n",
        "        self.train_set = {}\n",
        "        self.label_map = {}\n",
        "        self.name_map = {}\n",
        "\n",
        "    def add_image(self, image, label):\n",
        "        train_image = self.convert_to_tf_image(image)\n",
        "        self.add_label(label)\n",
        "        self.train_set[label] += [train_image]\n",
        "\n",
        "    def add_label(self, label):\n",
        "        if label in self.train_set:\n",
        "            return\n",
        "        n_labels = len(self.label_map)\n",
        "        self.train_set[label] = []\n",
        "        self.label_map[label] = n_labels\n",
        "        self.name_map[n_labels] = label\n",
        "\n",
        "    def get_batch(self, batch_size=16):\n",
        "        labels = []\n",
        "        images = []\n",
        "        for i in range(batch_size):\n",
        "            labels += [np.random.choice(list(self.name_map.keys()))]\n",
        "            examples = self.train_set[self.name_map[labels[-1]]]\n",
        "            images += random.sample(examples, 1)\n",
        "        return np.stack(images), np.stack(labels)\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_to_tf_image(image):\n",
        "        train_image = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), (224, 224))\n",
        "        train_image = tf.cast(train_image, tf.float32)\n",
        "        train_image = preprocess_input(train_image)\n",
        "        return train_image\n",
        "  \n",
        "class DatasetGUI:\n",
        "  def __init__(self, dataset, webcam):\n",
        "    self.dataset = dataset\n",
        "    self.webcam = webcam\n",
        "    self.build_widgets()\n",
        "    self.js_textfield = LiveJavascriptTextField()\n",
        "  \n",
        "  def build_widgets(self):\n",
        "    self.select = widgets.Select(\n",
        "        options=list(self.dataset.label_map.keys()),\n",
        "        description='Selected:',\n",
        "        disabled=False\n",
        "    )\n",
        "    self.button = widgets.Button(\n",
        "        description='Add image',\n",
        "        disabled = len(self.dataset.label_map) == 0,\n",
        "        button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "        tooltip='Add image',\n",
        "        icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "    )\n",
        "    self.text_field = widgets.Text(\n",
        "        placeholder='Object label',\n",
        "        description='New object:',\n",
        "        disabled=False\n",
        "    )\n",
        "    self.select.observe(self.onSelect, 'value')\n",
        "    self.text_field.on_submit(self.addObject)\n",
        "    self.button.on_click(self.addImage)\n",
        "\n",
        "  def addImage(self, _):\n",
        "    img = self.webcam.next()\n",
        "    self.dataset.add_image(img, self.select.value)\n",
        "    self.js_textfield.updateText('label : count')\n",
        "    for key, value in self.dataset.train_set.items():\n",
        "      self.js_textfield.addText(key + ':' + str(len(value)))\n",
        "\n",
        "  def onSelect(self, selected):\n",
        "    self.button.description = 'Add image to ' + selected['new']\n",
        "\n",
        "  def addObject(self, label):\n",
        "    if label.value in self.dataset.label_map:\n",
        "        return\n",
        "    self.dataset.add_label(label.value)\n",
        "    label.value = ''\n",
        "    self.select.options = list(self.dataset.label_map.keys())\n",
        "    self.button.disabled = False\n",
        "    self.select.disabled = False\n",
        "  \n",
        "  def display(self):\n",
        "    display(self.text_field, self.select, self.button)\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "class LiveJavascriptTextField:\n",
        "  def __init__(self, style=''):\n",
        "    self.initText(style)\n",
        "\n",
        "  def initText(self, style):\n",
        "    js = Javascript('''\n",
        "      that = this;\n",
        "      async function initText(style)\n",
        "      {\n",
        "        const video = document.querySelector(\"#output-area\");\n",
        "        const div = document.createElement('div');\n",
        "        div.innerHTML = '';\n",
        "        that.text_area = div;\n",
        "        div.style = style;\n",
        "        video.appendChild(\n",
        "          div\n",
        "          );\n",
        "      }\n",
        "      async function updateText(text)\n",
        "      {\n",
        "        that.text_area.innerHTML = text;\n",
        "      }\n",
        "      async function addText(text)\n",
        "      {\n",
        "        that.text_area.innerHTML += '<p>' + text + '</p>';\n",
        "      }\n",
        "    ''')\n",
        "    display(js);\n",
        "    eval_js('initText(\"{}\")'.format(style));\n",
        "  \n",
        "  def addText(self, text):\n",
        "    eval_js('addText(\"{}\")'.format(text))\n",
        "  \n",
        "  def updateText(self, text):\n",
        "    eval_js('updateText(\"{}\")'.format(text))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}